{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-43ffc52b1cda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Import packages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrandom\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "#Import packages\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "from random import randint, random,sample\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import translate\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from matplotlib import pyplot\n",
    "import math\n",
    "from time import time\n",
    "from numpy import unique\n",
    "from pandas import DataFrame\n",
    "import pickle\n",
    "from Bio.Seq import Seq\n",
    "from Bio.Alphabet import generic_dna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S.No</th>\n",
       "      <th>forward_primer</th>\n",
       "      <th>For_seq</th>\n",
       "      <th>reverse_primer</th>\n",
       "      <th>Rev_seq</th>\n",
       "      <th>Region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>119</td>\n",
       "      <td>AGYGGCGNACGGGTGAGTAA</td>\n",
       "      <td>338</td>\n",
       "      <td>TGCTGCCTCCCGTAGGAGT</td>\n",
       "      <td>V2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>357</td>\n",
       "      <td>CCTACGGGAGGCAGCAG</td>\n",
       "      <td>518</td>\n",
       "      <td>ATTACCGCGGCTGCTGG</td>\n",
       "      <td>V3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>577</td>\n",
       "      <td>AYTGGGYDTAAAGNG</td>\n",
       "      <td>785</td>\n",
       "      <td>TACNVGGGTATCTAATCC</td>\n",
       "      <td>V4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>785</td>\n",
       "      <td>AGGATTAGATACCCT</td>\n",
       "      <td>907</td>\n",
       "      <td>CCGTCAATTCCTTTGAGTTT</td>\n",
       "      <td>V5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>978</td>\n",
       "      <td>TCGAtGCAACGCGAAGAA</td>\n",
       "      <td>1062</td>\n",
       "      <td>ACATtTCACaACACGAGCTGACGA</td>\n",
       "      <td>V6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   S.No  forward_primer               For_seq  reverse_primer  \\\n",
       "0     1             119  AGYGGCGNACGGGTGAGTAA             338   \n",
       "1     2             357     CCTACGGGAGGCAGCAG             518   \n",
       "2     3             577       AYTGGGYDTAAAGNG             785   \n",
       "3     4             785       AGGATTAGATACCCT             907   \n",
       "4     5             978    TCGAtGCAACGCGAAGAA            1062   \n",
       "\n",
       "                     Rev_seq Region  \n",
       "0        TGCTGCCTCCCGTAGGAGT     V2  \n",
       "1          ATTACCGCGGCTGCTGG     V3  \n",
       "2         TACNVGGGTATCTAATCC     V4  \n",
       "3       CCGTCAATTCCTTTGAGTTT     V5  \n",
       "4  ACATtTCACaACACGAGCTGACGAÂ      V6  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reading the High variable region data used in the RandomForest classifier (16S Classifier)\n",
    "#to exclude the conserved region sequences\n",
    "HVR  = pd.read_csv('HVR.csv')\n",
    "HVR.columns = ['S.No','forward_primer','For_seq','reverse_primer','Rev_seq','Region']\n",
    "HVR.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Reading a small subset of the SILVA databae\n",
    "#For full training download this \n",
    "# https://www.arb-silva.de/fileadmin/silva_databases/release_132/Exports/SILVA_132_SSURef_Nr99_tax_silva.fasta.gz\n",
    "path = 'SILVA_132_dump.fasta'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Determine the maximum length of any sequence to add zeros in the end (zero padding)\n",
    "#max_len = max(df['len'])\n",
    "max_len=4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def zero_pad(list1):\n",
    "    x = np.pad(list1, (0,max_len-len(list1)), 'constant', constant_values=0)\n",
    "    return x.astype('uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##Function for Nucleotide sequence one hot encoding\n",
    "#1 Declare the alphabet\n",
    "alphabet = 'ACGTN'\n",
    "integer = [1,2,3,4,5]\n",
    "#2 Declare mapping functions\n",
    "char_to_int = {'A':1,'C':2,'G':3,'U':4,'R':0,'Y':0,'S':0,\n",
    "               'W':0,'K':0,'M':0,'B':0,'D':0,'H':0,'V':0,'N':5}\n",
    "int_to_char = {1:'A',2:'C',3:'G',4:'U',0:'R',0:'Y',0:'S',\n",
    "               0:'W',0:'K',0:'M',0:'B',0:'D',0:'H',0:'V',5:'N'}\n",
    "\n",
    "def encode_nu(sequence,n_features=4):\n",
    "    #3 convert char to number\n",
    "    encoded = [char_to_int[char] for char in sequence]\n",
    "    return array(encoded)#.astype('uint8')\n",
    "\n",
    "# Decode a encoded string\n",
    "def decode_nu(encoded):\n",
    "    decoded =  ''\n",
    "    decoded = [int_to_char[integ] for integ in encoded]\n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def encode_pad(seq):\n",
    "    return zero_pad(encode_nu(str(record.seq)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Reading the database as a panda dataframe\n",
    "reads=[]\n",
    "for record in SeqIO.parse(path, \"fasta\"):\n",
    "    str_ = str(record.description).split(\" \", 1)[1]\n",
    "    encoded = record.seq \n",
    "    #encoded = zero_pad(encode_nu(str(record.seq)))\n",
    "    \n",
    "    if str_.count(';') == 6:\n",
    "        [kingdom, phylum, class_,order,family,genus,species] = str_.split(';')\n",
    "    elif str_.count(';') > 6:    \n",
    "        list =  str_.split(';')\n",
    "        [kingdom,phylum, class_,order,family,genus,species] = list[-7:]\n",
    "    else:\n",
    "        list =  str_.split(';')\n",
    "        num = len(list)\n",
    "        [kingdom,phylum, class_,order,family,genus,species][:num] = list\n",
    "        [kingdom,phylum, class_,order,family,genus,species][num:] = '_'*(7-num)\n",
    "    reads.append([record.name,\n",
    "                  encoded,\n",
    "                  kingdom ,phylum, class_,order,family,genus,species\n",
    "                  ,len(record.seq)])    \n",
    "    \n",
    "df=pd.DataFrame(reads,columns=['id','seq','kingdom','phylum','class_','order',\n",
    "                               'family','genus','species','len'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Applying zero padding and encoding\n",
    "df['seq'] = df['seq'].apply(encode_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique Genus classes :  557\n"
     ]
    }
   ],
   "source": [
    "print(\"The number of unique Genus classes : \"  ,len(np.unique(df['genus'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Converting classes into vectors\n",
    "df['kingdom'] = pd.factorize(df['kingdom'].values)[0]\n",
    "df['phylum'] = pd.factorize(df['phylum'].values)[0]\n",
    "df['class_'] = pd.factorize(df['class_'].values)[0]\n",
    "df['order'] = pd.factorize(df['order'].values)[0]\n",
    "df['family'] = pd.factorize(df['family'].values)[0]\n",
    "df['genus'] = pd.factorize(df['genus'].values)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(557, 176, 86, 34, 22)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique(df['genus'])),len(unique(df['family'])),len(unique(df['order'])),len(unique(df['class_'])), len(unique(df['phylum']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "counts = df['phylum'].value_counts()\n",
    "df = df[df['phylum'].isin(counts[counts > 2].index)]\n",
    "counts = df['class_'].value_counts()\n",
    "df = df[df['class_'].isin(counts[counts > 2].index)]\n",
    "counts = df['order'].value_counts()\n",
    "df = df[df['order'].isin(counts[counts > 2].index)]\n",
    "counts = df['family'].value_counts()\n",
    "df = df[df['family'].isin(counts[counts > 2].index)]\n",
    "counts = df['genus'].value_counts()\n",
    "df = df[df['genus'].isin(counts[counts > 2].index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(148, 68, 37, 13, 10)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique(df['genus'])),len(unique(df['family'])),len(unique(df['order'])),len(unique(df['class_'])), len(unique(df['phylum']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2330, 10)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply multi outupt stratified sampling to ensure homogenous distribution to all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_all_labels(instances):\n",
    "    all_labels = set()\n",
    "    for x in instances:\n",
    "        all_labels |= set(x)\n",
    "    return all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def stratified_cross_validation_split(instances, k=10, r=None):\n",
    "    \"\"\"Creates a stratified cross-validation split of the given dataset as described in [1,2].\n",
    "    [1] http://lpis.csd.auth.gr/publications/sechidis-ecmlpkdd-2011.pdf\n",
    "    [2] https://de.slideshare.net/tsoumakas/on-the-stratification-of-multilabel-data\n",
    "    Args:\n",
    "        instances (list): Is a list of lists. For example [[1, 2, 3], [4,5]] means that the first instance has labels [1,2,3] attached, and the second item has the labels [4,5].\n",
    "        k (int, optional): The number of folds\n",
    "        r (None, optional): Weighting of sets. See paper\n",
    "    Returns:\n",
    "        list: A list with k lists. The items in one of the k lists are the indices of the instances. So when the list [[1, 3], [2,5]] is returned, the first set has the items [1, 3] and the second set has [2, 5] as instances. The number in the set are the indices of the instance in the \"instances\" parameter.\n",
    "    \"\"\"\n",
    "    labels = get_all_labels(instances)\n",
    "\n",
    "    num_labels = len(labels)\n",
    "    num_instances = len(instances)\n",
    "\n",
    "    assert(num_labels > 0)\n",
    "    assert(num_instances > 0)\n",
    "    assert(k > 0 and k <= num_instances)\n",
    "\n",
    "    if not r:\n",
    "        r = [1 / k] * k\n",
    "\n",
    "    # Calculate the desired number of examples at each subset\n",
    "    c_1 = [math.floor(num_instances * x) for x in r]\n",
    "    c_2 = {}\n",
    "    # Calculate the desired number of examples of each label at each subset\n",
    "    def get_D(used_instances):\n",
    "        D = {}\n",
    "        for idx, instance in enumerate(instances):\n",
    "            for label in instance:\n",
    "                if label not in D:\n",
    "                    D[label] = []\n",
    "                D[label].append(idx)\n",
    "        # for label in labels:\n",
    "        #    # Find the examples of each label in the initial set\n",
    "        #    D_2[label] = [idx for idx, x in enumerate(instances) if label in x and idx not in used_instances]\n",
    "        return D\n",
    "\n",
    "    # Most expensive\n",
    "    D = get_D(set())\n",
    "    for label, vals in D.items():\n",
    "        c_2[label] = [len(vals) * x for x in r]\n",
    "\n",
    "    used_instances = set()\n",
    "    S = [[] for x in range(k)]\n",
    "    counter = 0\n",
    "    checkpoints = [x * num_instances / k for x in range(k)]\n",
    "    print('Starting sorting elements into sets')\n",
    "    while len(used_instances) < len(instances):\n",
    "        if len(checkpoints) and len(used_instances) > checkpoints[0]:\n",
    "            print('{:>2.0f}%'.format(len(used_instances) / len(instances) * 100))\n",
    "            checkpoints = checkpoints[1:] if len(checkpoints) else []\n",
    "        counter += 1\n",
    "        # Find the label with the fewest (but at least one) remaining examples, breaking ties randomly\n",
    "        def sort_fn(x):\n",
    "            return len(x[1])\n",
    "        l = min(D.items(), key=sort_fn)\n",
    "        l = l[0]\n",
    "        for inst in D[l]:\n",
    "            M = sorted(enumerate(c_2[l]), key=lambda x: x[1])\n",
    "            M = [(idx, examples_wished) for idx, examples_wished in M if examples_wished == M[-1][1]]\n",
    "            if len(M) == 1:\n",
    "                m = M[0][0]\n",
    "            else:\n",
    "                idxs = [x[0] for x in M]\n",
    "                np.random.shuffle(idxs)\n",
    "                s = sorted([(idx, x) for idx, x in enumerate(c_1) if idx in idxs], key=lambda x: [1])\n",
    "                s = [idx for idx, x in s if x == s[-1][1]]\n",
    "                if len(s) == 1:\n",
    "                    m = s[0]\n",
    "                else:\n",
    "                    m = s[np.random.randint(0, len(s))]\n",
    "            # Find the subset(s) with the largest number of desired examples for this\n",
    "            # label, breaking ties by considering the largest number of desired\n",
    "            # examples, breaking further ties randomly\n",
    "            Y = instances[inst]\n",
    "            S[m].append(inst)\n",
    "            used_instances.add(inst)\n",
    "            # Update desired number of examples\n",
    "            for label in Y:\n",
    "                if inst in D[label]:\n",
    "                    # Instead of updating D globally on each iteration, update it on each instance\n",
    "                    D[label].remove(inst)\n",
    "                    if len(D[label]) == 0:\n",
    "                        # Delete label from D so that the sorting gets faster\n",
    "                        del D[label]\n",
    "                c_2[label][m] -= 1\n",
    "            c_1[m] -= 1\n",
    "    print(\"StratifyIterations: {}\".format(counter + 1))\n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting sorting elements into sets\n",
      " 0%\n",
      "33%\n",
      "69%\n",
      "StratifyIterations: 547\n"
     ]
    }
   ],
   "source": [
    "#x = [[1,2,3,12,11,11,13], [4,5,15,15,15,15],[13,15,72,72,15,13]]\n",
    "x= df.iloc[:,2:7]\n",
    "y = np.array(x).tolist()\n",
    "#print(x)\n",
    "np.random.seed(2)\n",
    "split = stratified_cross_validation_split(y,k=3,r=(0.7,0.1,0.2))\n",
    "\n",
    "splitting_index = open('split_test.txt', 'w')\n",
    "for item in split:\n",
    "    splitting_index.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the splitting index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    with open('split__'+str(i), 'wb') as fp:\n",
    "        pickle.dump(split[i], fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "split_list = []\n",
    "for i in range(3):\n",
    "    with open ('split__'+str(i), 'rb') as fp:\n",
    "        split_list.append(pickle.load(fp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "split_list = split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build Train , test , valid data from the slitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = df.iloc[split_list[0],:]\n",
    "valid = df.iloc[split_list[1],:]\n",
    "test = df.iloc[split_list[2],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3632, 10), (1033, 10), (519, 10))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape, valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Ensuring all data have the same labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = train[train['family'].isin(unique(valid['family']))]\n",
    "test = test[test['family'].isin(unique(valid['family']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = train[train['genus'].isin(unique(valid['genus']))]\n",
    "test = test[test['genus'].isin(unique(valid['genus']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = train[train['family'].isin(unique(test['family']))]\n",
    "valid = valid[valid['family'].isin(unique(test['family']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = train[train['genus'].isin(unique(test['genus']))]\n",
    "valid = valid[valid['genus'].isin(unique(test['genus']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test = test[test['genus'].isin(unique(train['genus']))]\n",
    "valid = valid[valid['genus'].isin(unique(train['genus']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(114, 114, 114)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(test['genus'])),len(np.unique(train['genus'])),len(np.unique(valid['genus']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 63, 63)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(test['family'])),len(np.unique(train['family'])),len(np.unique(valid['family']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31, 31, 31)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(test['order'])),len(np.unique(train['order'])),len(np.unique(valid['order']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10, 10)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(test['class_'])),len(np.unique(train['class_'])),len(np.unique(valid['class_']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 7, 7)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(test['phylum'])),len(np.unique(train['phylum'])),len(np.unique(valid['phylum']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Data normaliztion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train = train['seq']/5\n",
    "x_test  = test['seq']/5\n",
    "x_valid = valid['seq']/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train = np.concatenate(x_train.values).reshape(x_train.shape[0],max_len).tolist()\n",
    "x_test = np.concatenate(x_test.values).reshape(x_test.shape[0],max_len).tolist()\n",
    "x_valid = np.concatenate(x_valid.values).reshape(x_valid.shape[0],max_len).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Convering each label to categorical\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "y1_train = to_categorical(train['phylum'])\n",
    "y1_test = to_categorical(test['phylum'])\n",
    "y1_valid = to_categorical(valid['phylum'])\n",
    "\n",
    "y2_train = to_categorical(train['class_'])\n",
    "y2_test = to_categorical(test['class_'])\n",
    "y2_valid = to_categorical(valid['class_'])\n",
    "\n",
    "y3_train = to_categorical(train['order'])\n",
    "y3_test = to_categorical(test['order'])\n",
    "y3_valid = to_categorical(valid['order'])\n",
    "\n",
    "y4_train = to_categorical(train['family'])\n",
    "y4_test = to_categorical(test['family'])\n",
    "y4_valid = to_categorical(valid['family'])\n",
    "\n",
    "y5_train = to_categorical(train['genus'])\n",
    "y5_test = to_categorical(test['genus'])\n",
    "y5_valid = to_categorical(valid['genus'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_train = [y1_train,y2_train,y3_train,y4_train,y5_train]\n",
    "y_test = [y1_test,y2_test,y3_test,y4_test,y5_test]\n",
    "y_valid= [y1_valid,y2_valid,y3_valid,y4_valid,y5_valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2870, 20)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(439, 20)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_valid[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(848, 20)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Embedding, Input, Masking, Dropout\n",
    "from keras.layers import LSTM, TimeDistributed, AveragePooling1D, Flatten, TimeDistributed\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils.training_utils import multi_gpu_model\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping,ModelCheckpoint, CSVLogger\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, ConvLSTM2D, Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Uncomment for parallel GPU training\n",
    "import os\n",
    "##os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build LSTM_no_dropout model...\n"
     ]
    }
   ],
   "source": [
    "# Model1\n",
    "#LSTM_Masked_no_dropout\n",
    "\n",
    "print('Build LSTM_no_dropout model...')\n",
    "\n",
    "input1 = Input(shape=(max_len,))\n",
    "m = Embedding(6, 50)(input1)\n",
    "m = Masking(mask_value=0)(m)\n",
    "m = LSTM(32,return_sequences=False)(m)\n",
    "output1 = Dense(y1_train.shape[1],activation='softmax', name='output1')(m)\n",
    "output2 = Dense(y2_train.shape[1],activation='softmax', name='output2')(m)\n",
    "output3 = Dense(y3_train.shape[1],activation='softmax', name='output3')(m)\n",
    "output4 = Dense(y4_train.shape[1],activation='softmax', name='output4')(m)\n",
    "output5 = Dense(y5_train.shape[1],activation='softmax', name='output5')(m)\n",
    "\n",
    "m = Model(inputs=[input1], outputs=[output1, output2, output3,output4, output5])\n",
    "\n",
    "#m = multi_gpu_model(m, gpus=4)\n",
    "\n",
    "optimizer = Adam(lr=0.0001)\n",
    "m.compile(loss='categorical_crossentropy',optimizer=optimizer,metrics=['acc']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m.fit(np.array(x_train),\n",
    "      [y1_train,y2_train,y3_train,y4_train,y5_train],\n",
    "      batch_size=10,\n",
    "      validation_data=(np.array(x_valid),y_valid),\n",
    "      epochs=95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build CNN_no_dropout model...\n"
     ]
    }
   ],
   "source": [
    "# Model2\n",
    "#CNN_no_dropout\n",
    "\n",
    "print('Build CNN_no_dropout model...')\n",
    "\n",
    "input1 = Input(shape=(max_len,), dtype='uint8')\n",
    "m = Embedding(5, 32)(input1)\n",
    "m = Conv1D(16,(3))(m)\n",
    "m = AveragePooling1D(8,2)(m)\n",
    "m = Flatten()(m)\n",
    "#m = Dense(32)(m)\n",
    "output1 = Dense(y1_train.shape[1],activation='softmax', name='output1')(m)\n",
    "output2 = Dense(y2_train.shape[1],activation='softmax', name='output2')(m)\n",
    "output3 = Dense(y3_train.shape[1],activation='softmax', name='output3')(m)\n",
    "output4 = Dense(y4_train.shape[1],activation='softmax', name='output4')(m)\n",
    "output5 = Dense(y5_train.shape[1],activation='softmax', name='output5')(m)\n",
    "\n",
    "m = Model(inputs=[input1], outputs=[output1, output2, output3,output4, output5])\n",
    "\n",
    "#m = multi_gpu_model(m, gpus=4)\n",
    "optimizer = Adam(lr=0.00001)\n",
    "m.compile(loss='categorical_crossentropy',optimizer=optimizer,metrics=['acc']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7dbe6eeac8>"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath = ''.join('./smallRun/CNN.hdfs')\n",
    "Checkpoint = ModelCheckpoint(filepath,monitor='val_loss',\n",
    "                             verbose=0, save_best_only=True,\n",
    "                             save_weights_only=True, mode='auto', period=1)\n",
    "m.fit(np.array(x_train),y_train,callbacks=[Checkpoint],verbose=0,\n",
    "      batch_size=10,validation_data=(np.array(x_valid),y_valid),\n",
    "      epochs=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.load_weights('./smallRun/CNN.hdfs')\n",
    "pred = m.predict(np.array(x_test))\n",
    "accuracy_score(y_test[4].argmax(axis=1),pred[4].argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build Bi-LSTM model...\n"
     ]
    }
   ],
   "source": [
    "# Model3\n",
    "#BILSTM_no_dropout\n",
    "\n",
    "print('Build Bi-LSTM model...')\n",
    "\n",
    "input1 = Input(shape=(max_len,), dtype='uint8')\n",
    "m = Embedding(6, 32, name='embedding')(input1)\n",
    "m = Masking(mask_value=0)(m)\n",
    "m = Bidirectional(LSTM(32,return_sequences=True))(m)\n",
    "#m = Conv1D(32,(2))(m)\n",
    "#m = Flatten()(m)\n",
    "#m = GlobalMaxPooling1D()(m)\n",
    "#m = TimeDistributed(layer=)(m)\n",
    "output1 = Dense(y1_train.shape[1],activation='softmax', name='output1')(m)\n",
    "output2 = Dense(y2_train.shape[1],activation='softmax', name='output2')(m)\n",
    "output3 = Dense(y3_train.shape[1],activation='softmax', name='output3')(m)\n",
    "output4 = Dense(y4_train.shape[1],activation='softmax', name='output4')(m)\n",
    "output5 = Dense(y5_train.shape[1],activation='softmax', name='output5')(m)\n",
    "\n",
    "m = Model(inputs=[input1], outputs=[output1, output2, output3,output4, output5])\n",
    "\n",
    "#m = multi_gpu_model(m, gpus=4)\n",
    "optimizer = Adam(lr=0.0001)\n",
    "m.compile(loss='categorical_crossentropy',optimizer=optimizer,metrics=['acc']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m.fit(np.array(x_train),\n",
    "      [y1_train,y2_train,y3_train,y4_train,y5_train],\n",
    "      batch_size=10,\n",
    "      validation_data=(np.array(x_valid),y_valid),\n",
    "      epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filepath = ''.join('./bestModel/All_128_batch.hdfs')\n",
    "Checkpoint = ModelCheckpoint(filepath,#monitor='val_loss',\n",
    "                             verbose=0, save_best_only=True,\n",
    "                             save_weights_only=True, mode='auto', period=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build CNN_no_dropout model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.14356435643564355"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model4\n",
    "#CNN_no_dropout_genus_only\n",
    "input1 = Input(shape=(max_len,), dtype='uint8')\n",
    "m = Embedding(5, 32)(input1)\n",
    "m = Conv1D(16,(3))(m)\n",
    "m = AveragePooling1D(8,2)(m)\n",
    "m = Flatten()(m)\n",
    "output5 = Dense(y5_train.shape[1],activation='softmax', name='output5')(m)\n",
    "m = Model(inputs=[input1], outputs=[output5])\n",
    "optimizer = Adam(lr=0.00001)\n",
    "m.compile(loss='categorical_crossentropy',optimizer=optimizer,metrics=['acc'])\n",
    "filepath = ''.join('./smallRun/CNN_genus.hdfs')\n",
    "Checkpoint = ModelCheckpoint(filepath,monitor='val_loss',\n",
    "                             verbose=0, save_best_only=True,\n",
    "                             save_weights_only=True, mode='auto', period=1)\n",
    "csv_logger = CSVLogger('smallRun/CNN_genus.log')\n",
    "m.fit(np.array(x_train),y5_train,callbacks=[Checkpoint,csv_logger],verbose=0,\n",
    "      batch_size=10,validation_data=(np.array(x_valid),y5_valid),\n",
    "      epochs=100)\n",
    "m.load_weights('./smallRun/CNN_genus.hdfs')\n",
    "pred = m.predict(np.array(x_test))\n",
    "accuracy_score(y5_test.argmax(axis=1),pred.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Model5\n",
    "#CNN_dropout_genus_only\n",
    "input1 = Input(shape=(max_len,), dtype='uint8')\n",
    "m = Embedding(5, 32)(input1)\n",
    "m = Conv1D(16,(3))(m)\n",
    "m = AveragePooling1D(8,2)(m)\n",
    "m = Flatten()(m)\n",
    "m = Dropout(0.2)(m)\n",
    "output5 = Dense(y5_train.shape[1],activation='softmax', name='output5')(m)\n",
    "m = Model(inputs=[input1], outputs=[output5])\n",
    "optimizer = Adam(lr=0.00001)\n",
    "m.compile(loss='categorical_crossentropy',optimizer=optimizer,metrics=['acc'])\n",
    "filepath = ''.join('./smallRun/CNN_genus.hdfs')\n",
    "Checkpoint = ModelCheckpoint(filepath,monitor='val_loss',\n",
    "                             verbose=0, save_best_only=True,\n",
    "                             save_weights_only=True, mode='auto', period=1)\n",
    "csv_logger = CSVLogger('smallRun/CNN_drop_genus.log')\n",
    "m.fit(np.array(x_train),y5_train,callbacks=[Checkpoint,csv_logger],verbose=0,\n",
    "      batch_size=10,validation_data=(np.array(x_valid),y5_valid),\n",
    "      epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14356435643564355"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.load_weights('./smallRun/CNN_genus.hdfs')\n",
    "pred = m.predict(np.array(x_test))\n",
    "accuracy_score(y5_test.argmax(axis=1),pred.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14356435643564355"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model6\n",
    "#Stacked_CNN_genus\n",
    "input1 = Input(shape=(max_len,), dtype='uint8')\n",
    "m = Embedding(5, 32)(input1)\n",
    "m = Conv1D(16,(3))(m)\n",
    "m = AveragePooling1D(8,2)(m)\n",
    "m = Conv1D(16,(3))(m)\n",
    "m = AveragePooling1D(8,2)(m)\n",
    "m = Flatten()(m)\n",
    "output5 = Dense(y5_train.shape[1],activation='softmax', name='output5')(m)\n",
    "m = Model(inputs=[input1], outputs=[output5])\n",
    "optimizer = Adam(lr=0.00001)\n",
    "m.compile(loss='categorical_crossentropy',optimizer=optimizer,metrics=['acc'])\n",
    "filepath = ''.join('./smallRun/Stacked_CNN_genus.hdfs')\n",
    "Checkpoint = ModelCheckpoint(filepath,monitor='val_loss',\n",
    "                             verbose=0, save_best_only=True,\n",
    "                             save_weights_only=True, mode='auto', period=1)\n",
    "csv_logger = CSVLogger('smallRun/Stacked_CNN_genus.log')\n",
    "m.fit(np.array(x_train),y5_train,callbacks=[Checkpoint,csv_logger],verbose=0,\n",
    "      batch_size=10,validation_data=(np.array(x_valid),y5_valid),\n",
    "      epochs=100)\n",
    "m.load_weights('./smallRun/Stacked_CNN_genus.hdfs')\n",
    "pred = m.predict(np.array(x_test))\n",
    "accuracy_score(y5_test.argmax(axis=1),pred.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14356435643564355"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model7\n",
    "#3_Stacked_CNN_genus\n",
    "\n",
    "input1 = Input(shape=(max_len,), dtype='uint8')\n",
    "m = Embedding(5, 32)(input1)\n",
    "m = Conv1D(16,(3))(m)\n",
    "m = AveragePooling1D(8,2)(m)\n",
    "m = Conv1D(16,(3))(m)\n",
    "m = AveragePooling1D(8,2)(m)\n",
    "m = Conv1D(16,(3))(m)\n",
    "m = AveragePooling1D(8,2)(m)\n",
    "m = Flatten()(m)\n",
    "#m = Dense(32)(m)\n",
    "\n",
    "output5 = Dense(y5_train.shape[1],activation='softmax', name='output5')(m)\n",
    "\n",
    "m = Model(inputs=[input1], outputs=[output5])\n",
    "\n",
    "optimizer = Adam(lr=0.00001)\n",
    "m.compile(loss='categorical_crossentropy',optimizer=optimizer,metrics=['acc'])\n",
    "filepath = ''.join('./smallRun/3Stacked_CNN_genus.hdfs')\n",
    "Checkpoint = ModelCheckpoint(filepath,monitor='val_loss',\n",
    "                             verbose=0, save_best_only=True,\n",
    "                             save_weights_only=True, mode='auto', period=1)\n",
    "csv_logger = CSVLogger('smallRun/3Stacked_CNN_genus.log')\n",
    "m.fit(np.array(x_train),y5_train,callbacks=[Checkpoint,csv_logger],verbose=0,\n",
    "      batch_size=10,validation_data=(np.array(x_valid),y5_valid),\n",
    "      epochs=100)\n",
    "m.load_weights('./smallRun/3Stacked_CNN_genus.hdfs')\n",
    "pred = m.predict(np.array(x_test))\n",
    "accuracy_score(y5_test.argmax(axis=1),pred.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14356435643564355"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model8\n",
    "#Stacked_drop_CNN_genus\n",
    "\n",
    "input1 = Input(shape=(max_len,), dtype='uint8')\n",
    "m = Embedding(5, 32)(input1)\n",
    "m = Conv1D(16,(3))(m)\n",
    "m = AveragePooling1D(8,2)(m)\n",
    "m = Dropout(0.2)(m)\n",
    "m = Conv1D(16,(3))(m)\n",
    "m = AveragePooling1D(8,2)(m)\n",
    "m = Dropout(0.2)(m)\n",
    "m = Flatten()(m)\n",
    "output5 = Dense(y5_train.shape[1],activation='softmax', name='output5')(m)\n",
    "\n",
    "m = Model(inputs=[input1], outputs=[output5])\n",
    "\n",
    "optimizer = Adam(lr=0.00001)\n",
    "m.compile(loss='categorical_crossentropy',optimizer=optimizer,metrics=['acc'])\n",
    "filepath = ''.join('./smallRun/Stacked_drop_CNN_genus.hdfs')\n",
    "Checkpoint = ModelCheckpoint(filepath,monitor='val_loss',\n",
    "                             verbose=0, save_best_only=True,\n",
    "                             save_weights_only=True, mode='auto', period=1)\n",
    "csv_logger = CSVLogger('smallRun/Stacked_drop_CNN_genus.log')\n",
    "m.fit(np.array(x_train),y5_train,callbacks=[Checkpoint,csv_logger],verbose=0,\n",
    "      batch_size=10,validation_data=(np.array(x_valid),y5_valid),\n",
    "      epochs=100)\n",
    "m.load_weights('./smallRun/Stacked_drop_CNN_genus.hdfs')\n",
    "pred = m.predict(np.array(x_test))\n",
    "accuracy_score(y5_test.argmax(axis=1),pred.argmax(axis=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
